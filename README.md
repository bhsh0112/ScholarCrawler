# Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…·

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ Google Scholar æ–‡çŒ®æœç´¢å’Œçˆ¬å–å·¥å…·ï¼Œå¯ä»¥æ ¹æ®å…³é”®å­—æœç´¢å­¦æœ¯æ–‡çŒ®ï¼ŒæŒ‰å¼•ç”¨é‡æ’åºï¼Œå¹¶å¯¼å‡ºä¸º CSV æ ¼å¼ã€‚

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

- ğŸ” **å…³é”®å­—æœç´¢**: æ”¯æŒä»»æ„å…³é”®å­—æœç´¢å­¦æœ¯æ–‡çŒ®
- ğŸ“Š **å¼•ç”¨é‡æ’åº**: è‡ªåŠ¨æŒ‰å¼•ç”¨é‡ä»é«˜åˆ°ä½æ’åº
- ğŸ¯ **æ™ºèƒ½ç­›é€‰**: æ”¯æŒæŒ‰æœ€å°å¼•ç”¨é‡ç­›é€‰æ–‡çŒ®
- ğŸ“ **CSVå¯¼å‡º**: å¯¼å‡ºç»“æ„åŒ–çš„ CSV è¡¨æ ¼æ–‡ä»¶
- ğŸŒ **ä»£ç†æ”¯æŒ**: æ”¯æŒä½¿ç”¨ä»£ç†é¿å…è®¿é—®é™åˆ¶
- ğŸ“ˆ **ç»Ÿè®¡åˆ†æ**: è‡ªåŠ¨ç”Ÿæˆæ–‡çŒ®ç»Ÿè®¡ä¿¡æ¯

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
cd scholar_crawler
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ç”¨æ³•

```bash
# æœç´¢"deep learning"ç›¸å…³æ–‡çŒ®
python scholar_crawler.py "deep learning"

# æœç´¢100ç¯‡æ–‡çŒ®
python scholar_crawler.py "machine learning" --max 100

# åªä¿ç•™å¼•ç”¨é‡>=50çš„æ–‡çŒ®
python scholar_crawler.py "computer vision" --min-citations 50

# æŒ‡å®šè¾“å‡ºæ–‡ä»¶å
python scholar_crawler.py "neural networks" --output my_results.csv

# ä½¿ç”¨ä»£ç†ï¼ˆæ¨èï¼Œé¿å…è¢«é™åˆ¶ï¼‰
python scholar_crawler.py "artificial intelligence" --proxy
```

### å®Œæ•´ç¤ºä¾‹

```bash
# æœç´¢"deep learning"ï¼Œè·å–100ç¯‡æ–‡çŒ®ï¼Œç­›é€‰å¼•ç”¨é‡>=100çš„ï¼Œä½¿ç”¨ä»£ç†
python scholar_crawler.py "deep learning" --max 100 --min-citations 100 --proxy --output dl_papers.csv
```

## ğŸ“‹ å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `keyword` | æœç´¢å…³é”®å­—ï¼ˆå¿…éœ€ï¼‰ | - |
| `--max` | æœ€å¤§è·å–æ–‡çŒ®æ•°é‡ | 50 |
| `--min-citations` | æœ€å°å¼•ç”¨é‡ç­›é€‰ | 0 |
| `--output` | è¾“å‡ºCSVæ–‡ä»¶å | è‡ªåŠ¨ç”Ÿæˆ |
| `--proxy` | ä½¿ç”¨ä»£ç† | ä¸ä½¿ç”¨ |

## ğŸ“Š è¾“å‡ºæ ¼å¼

CSVæ–‡ä»¶åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

| å­—æ®µ | è¯´æ˜ |
|------|------|
| title | è®ºæ–‡æ ‡é¢˜ |
| authors | ä½œè€…åˆ—è¡¨ï¼ˆåˆ†å·åˆ†éš”ï¼‰ |
| year | å‘è¡¨å¹´ä»½ |
| venue | å‘è¡¨ä¼šè®®/æœŸåˆŠ |
| publisher | å‡ºç‰ˆå•† |
| citations | å¼•ç”¨æ¬¡æ•° |
| abstract | æ‘˜è¦ |
| url | è®ºæ–‡é“¾æ¥ |
| eprint_url | é¢„å°æœ¬é“¾æ¥ |

## ğŸ“ è¾“å‡ºç¤ºä¾‹

```
results/
â”œâ”€â”€ deep_learning_20251020_143052.csv
â”œâ”€â”€ machine_learning_20251020_144201.csv
â””â”€â”€ computer_vision_20251020_145327.csv
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è®¿é—®é™åˆ¶**: Google Scholar å¯èƒ½ä¼šé™åˆ¶é¢‘ç¹è®¿é—®ï¼Œå»ºè®®ï¼š
   - ä½¿ç”¨ `--proxy` å‚æ•°å¯ç”¨ä»£ç†
   - åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ äº†2ç§’å»¶è¿Ÿ
   - ä¸è¦ä¸€æ¬¡æ€§çˆ¬å–è¿‡å¤šæ•°æ®

2. **ç½‘ç»œé—®é¢˜**: å¦‚æœé‡åˆ°ç½‘ç»œé”™è¯¯ï¼š
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - å°è¯•ä½¿ç”¨ä»£ç†
   - å‡å°‘ `--max` å‚æ•°å€¼

3. **æ•°æ®å‡†ç¡®æ€§**: 
   - å¼•ç”¨æ•°å¯èƒ½ä¸æ˜¯å®æ—¶æ›´æ–°
   - éƒ¨åˆ†å­—æ®µå¯èƒ½ä¸ºç©ºï¼ˆå¦‚æ‘˜è¦ã€é“¾æ¥ç­‰ï¼‰

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### åœ¨ Python ä»£ç ä¸­ä½¿ç”¨

```python
from scholar_crawler import ScholarCrawler

# åˆ›å»ºçˆ¬è™«å®ä¾‹
crawler = ScholarCrawler(use_proxy=True)

# æœç´¢æ–‡çŒ®
papers = crawler.search_papers("deep learning", max_results=50)

# æŒ‰å¼•ç”¨é‡æ’åº
papers = crawler.sort_by_citations(papers)

# ç­›é€‰é«˜å¼•ç”¨æ–‡çŒ®
papers = crawler.filter_by_citations(papers, min_citations=100)

# å¯¼å‡ºCSV
crawler.export_to_csv(papers, "results.csv", "deep learning")
```

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1: å®‰è£… scholarly å¤±è´¥

```bash
# å°è¯•å‡çº§ pip
pip install --upgrade pip

# é‡æ–°å®‰è£…
pip install scholarly
```

### é—®é¢˜2: æ— æ³•è·å–æ•°æ®

- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ä½¿ç”¨ `--proxy` å‚æ•°
- ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•

### é—®é¢˜3: ä»£ç†è®¾ç½®å¤±è´¥

å¦‚æœå…è´¹ä»£ç†ä¸ç¨³å®šï¼Œå¯ä»¥ï¼š
- ä¸ä½¿ç”¨ä»£ç†ï¼Œç›´æ¥è®¿é—®
- é…ç½®è‡ªå·±çš„ä»£ç†æœåŠ¡å™¨ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-10-20)
- åˆå§‹ç‰ˆæœ¬
- æ”¯æŒåŸºç¡€æœç´¢å’ŒCSVå¯¼å‡º
- æ”¯æŒå¼•ç”¨é‡ç­›é€‰å’Œæ’åº
- æ”¯æŒä»£ç†åŠŸèƒ½

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚

