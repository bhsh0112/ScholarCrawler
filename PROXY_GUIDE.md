# ä»£ç†è®¾ç½®æŒ‡å— ğŸŒ

## ä¸ºä»€ä¹ˆéœ€è¦ä»£ç†ï¼Ÿ

Google Scholar ä¼šé™åˆ¶é¢‘ç¹è®¿é—®ï¼Œä½¿ç”¨ä»£ç†å¯ä»¥ï¼š
- âœ… é¿å…IPè¢«å°ç¦
- âœ… æé«˜æœç´¢æˆåŠŸç‡
- âœ… ç»•è¿‡è®¿é—®é™åˆ¶
- âœ… æ›´ç¨³å®šçš„è¿æ¥

---

## æ–¹å¼ä¸€ï¼šä½¿ç”¨å†…ç½®å…è´¹ä»£ç†ï¼ˆæœ€ç®€å•ï¼‰

### ä½¿ç”¨æ–¹æ³•

åªéœ€åœ¨å‘½ä»¤åæ·»åŠ  `--proxy` å‚æ•°ï¼š

```bash
python scholar_crawler.py "deep learning" --proxy
```

æˆ–è€…ä¸å…¶ä»–å‚æ•°ç»„åˆï¼š

```bash
python scholar_crawler.py "deep learning" \
  --config configs/cv_top_conferences.json \
  --proxy
```

### å†…ç½®ä»£ç†è¯´æ˜

- ä½¿ç”¨ scholarly åº“çš„å…è´¹ä»£ç†åŠŸèƒ½
- è‡ªåŠ¨è½®æ¢ä»£ç†IP
- æ— éœ€é¢å¤–é…ç½®
- å¯èƒ½ä¸å¤ªç¨³å®šï¼ˆå…è´¹ä»£ç†çš„é€šç—…ï¼‰

### ç¤ºä¾‹

```bash
# åŸºç¡€æœç´¢ + ä»£ç†
python scholar_crawler.py "computer vision" --proxy --max 30

# é«˜çº§æ£€ç´¢ + ä»£ç†
python scholar_crawler.py "deep learning" \
  --year-start 2020 \
  --min-citations 50 \
  --proxy
```

---

## æ–¹å¼äºŒï¼šä½¿ç”¨è‡ªå®šä¹‰HTTPä»£ç†

å¦‚æœä½ æœ‰è‡ªå·±çš„ä»£ç†æœåŠ¡å™¨ï¼ˆå¦‚Clashã€V2Rayã€Shadowsocksç­‰ï¼‰ï¼Œå¯ä»¥é…ç½®è‡ªå®šä¹‰ä»£ç†ã€‚

### æ­¥éª¤1ï¼šä¿®æ”¹ config.py

æ‰“å¼€ `/Users/shanhao/Desktop/script/scholar_crawler/config.py`ï¼Œæ‰¾åˆ° `CUSTOM_PROXY` éƒ¨åˆ†ï¼š

```python
# ä»£ç†é…ç½®ï¼ˆå¦‚æœä½¿ç”¨è‡ªå®šä¹‰ä»£ç†ï¼‰
CUSTOM_PROXY = {
    'http': None,   # ä¾‹å¦‚: 'http://127.0.0.1:7890'
    'https': None,  # ä¾‹å¦‚: 'http://127.0.0.1:7890'
}
```

ä¿®æ”¹ä¸ºä½ çš„ä»£ç†åœ°å€ï¼š

```python
# ä»£ç†é…ç½®
CUSTOM_PROXY = {
    'http': 'http://127.0.0.1:7890',    # ä½ çš„HTTPä»£ç†åœ°å€
    'https': 'http://127.0.0.1:7890',   # ä½ çš„HTTPSä»£ç†åœ°å€
}
```

**å¸¸è§ä»£ç†ç«¯å£**ï¼š
- Clashï¼š`7890`
- V2Rayï¼š`10809` æˆ– `1080`
- Shadowsocksï¼š`1080`
- å…¶ä»–ï¼šæŸ¥çœ‹ä½ çš„ä»£ç†è½¯ä»¶è®¾ç½®

### æ­¥éª¤2ï¼šä¿®æ”¹ scholar_crawler.py

ç›®å‰ä»£ç ä½¿ç”¨çš„æ˜¯ scholarly å†…ç½®ä»£ç†ã€‚å¦‚æœè¦ä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼Œéœ€è¦ç¨å¾®ä¿®æ”¹ä»£ç ã€‚

åˆ›å»ºä¸€ä¸ªå¢å¼ºç‰ˆæœ¬ `scholar_crawler_enhanced.py`ï¼š

```python
import os

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ç¯å¢ƒå˜é‡è®¾ç½®
def setup_system_proxy():
    """è®¾ç½®ç³»ç»Ÿä»£ç†ç¯å¢ƒå˜é‡"""
    proxy_url = "http://127.0.0.1:7890"  # ä¿®æ”¹ä¸ºä½ çš„ä»£ç†åœ°å€
    
    os.environ['HTTP_PROXY'] = proxy_url
    os.environ['HTTPS_PROXY'] = proxy_url
    os.environ['http_proxy'] = proxy_url
    os.environ['https_proxy'] = proxy_url
    
    print(f"âœ“ ç³»ç»Ÿä»£ç†å·²è®¾ç½®: {proxy_url}")

# åœ¨ main() å‡½æ•°å¼€å§‹å¤„è°ƒç”¨
def main():
    # å¦‚æœéœ€è¦ä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # setup_system_proxy()
    
    # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜
```

---

## æ–¹å¼ä¸‰ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰

è¿™æ˜¯æœ€çµæ´»çš„æ–¹å¼ï¼Œä¸éœ€è¦ä¿®æ”¹ä»£ç ã€‚

### macOS/Linux

```bash
# ä¸´æ—¶è®¾ç½®ï¼ˆä»…å½“å‰ç»ˆç«¯æœ‰æ•ˆï¼‰
export HTTP_PROXY=http://127.0.0.1:7890
export HTTPS_PROXY=http://127.0.0.1:7890

# ç„¶åè¿è¡Œçˆ¬è™«
python scholar_crawler.py "deep learning" --config configs/cv_top_conferences.json

# æˆ–è€…ä¸€è¡Œå‘½ä»¤
HTTP_PROXY=http://127.0.0.1:7890 HTTPS_PROXY=http://127.0.0.1:7890 \
python scholar_crawler.py "deep learning" --proxy
```

### æ°¸ä¹…è®¾ç½®ï¼ˆå¯é€‰ï¼‰

ç¼–è¾‘ `~/.zshrc` æˆ– `~/.bash_profile`ï¼š

```bash
# æ·»åŠ ä»¥ä¸‹è¡Œ
export HTTP_PROXY=http://127.0.0.1:7890
export HTTPS_PROXY=http://127.0.0.1:7890

# ä¿å­˜åæ‰§è¡Œ
source ~/.zshrc
```

### Windows

```cmd
# CMD
set HTTP_PROXY=http://127.0.0.1:7890
set HTTPS_PROXY=http://127.0.0.1:7890

# PowerShell
$env:HTTP_PROXY="http://127.0.0.1:7890"
$env:HTTPS_PROXY="http://127.0.0.1:7890"

# ç„¶åè¿è¡Œ
python scholar_crawler.py "deep learning" --proxy
```

---

## æ–¹å¼å››ï¼šæ£€æŸ¥å¹¶ä½¿ç”¨ç³»ç»Ÿä»£ç†

### æŸ¥çœ‹å½“å‰ä»£ç†è®¾ç½®

```bash
# macOS/Linux
echo $HTTP_PROXY
echo $HTTPS_PROXY

# æˆ–è€…
env | grep -i proxy
```

### å¦‚æœå·²é…ç½®ç³»ç»Ÿä»£ç†

ç›´æ¥è¿è¡Œå³å¯ï¼ŒPythonä¼šè‡ªåŠ¨ä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼š

```bash
python scholar_crawler.py "deep learning" --config configs/cv_top_conferences.json
```

---

## ğŸ”§ é’ˆå¯¹ä½ å½“å‰é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ

æ ¹æ®ä½ çš„é”™è¯¯ä¿¡æ¯ï¼Œé—®é¢˜ä¸ä»…æ˜¯ä»£ç†ï¼Œè¿˜æœ‰**ç­›é€‰æ¡ä»¶å¤ªä¸¥æ ¼**ã€‚

### é—®é¢˜åˆ†æ

```
âš  å·²æœç´¢ 150 ç¯‡ï¼Œä½†åªæ‰¾åˆ° 0 ç¯‡ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ®
âœ“ æˆåŠŸè·å– 0 ç¯‡æ–‡çŒ®ï¼ˆç­›é€‰æ‰ 150 ç¯‡ä¸ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ®ï¼‰
```

è¿™è¯´æ˜ï¼š
1. **æœç´¢æˆåŠŸ**äº†ï¼ˆæ‰¾åˆ°150ç¯‡ï¼‰
2. **å…¨éƒ¨è¢«ç­›é€‰æ¡ä»¶è¿‡æ»¤æ‰**äº†

### è§£å†³æ–¹æ¡ˆ1ï¼šæ”¾å®½ç­›é€‰æ¡ä»¶

æŸ¥çœ‹ä½ çš„é…ç½® `cv_top_conferences.json`ï¼š

```json
{
  "year_start": 2020,
  "citations_min": 30,
  "venues": ["CVPR", "ICCV", "ECCV"],
  "exclude_keywords": ["survey", "review"]
}
```

é—®é¢˜å¯èƒ½æ˜¯ï¼š
- âœ— venues ç­›é€‰å¤ªä¸¥æ ¼ï¼ˆå¿…é¡»åŒ…å«è¿™äº›ä¼šè®®åï¼‰
- âœ— citations_min å¯èƒ½å¤ªé«˜ï¼ˆæ–°æ–‡çŒ®å¼•ç”¨å°‘ï¼‰

**ä¿®æ”¹å»ºè®®**ï¼š

åˆ›å»ºæ–°é…ç½® `cv_top_conferences_relaxed.json`ï¼š

```json
{
  "description": "CVé¡¶ä¼šé…ç½®ï¼ˆæ”¾å®½ç‰ˆï¼‰",
  "year_start": 2020,
  "year_end": null,
  "citations_min": 10,  // é™ä½åˆ°10
  "citations_max": null,
  "authors": [],
  "publishers": [],
  "venues": [],  // å…ˆä¸é™åˆ¶venueï¼Œçœ‹çœ‹ç»“æœ
  "additional_keywords": ["CVPR", "ICCV", "ECCV"],  // æ”¹ä¸ºé¢å¤–å…³é”®å­—
  "exclude_keywords": [],  // å…ˆä¸æ’é™¤
  "keyword_mode": "OR"
}
```

### è§£å†³æ–¹æ¡ˆ2ï¼šé€æ­¥æ·»åŠ æ¡ä»¶

```bash
# ç¬¬ä¸€æ­¥ï¼šåŸºç¡€æœç´¢ï¼Œä¸åŠ é™åˆ¶
python scholar_crawler.py "deep learning computer vision" --max 20

# ç¬¬äºŒæ­¥ï¼šæ·»åŠ å¹´ä»½é™åˆ¶
python scholar_crawler.py "deep learning computer vision" \
  --year-start 2020 --max 20

# ç¬¬ä¸‰æ­¥ï¼šæ·»åŠ å¼•ç”¨é‡ï¼ˆä»ä½å¼€å§‹ï¼‰
python scholar_crawler.py "deep learning computer vision" \
  --year-start 2020 --min-citations 10 --max 20

# ç¬¬å››æ­¥ï¼šå¦‚æœç»“æœåˆé€‚ï¼Œå†æ·»åŠ å…¶ä»–æ¡ä»¶
```

### è§£å†³æ–¹æ¡ˆ3ï¼šä½¿ç”¨ä»£ç† + æ”¾å®½æ¡ä»¶

```bash
# æ¨èæ–¹å¼
python scholar_crawler.py "deep learning computer vision" \
  --year-start 2020 \
  --min-citations 10 \
  --max 30 \
  --proxy
```

---

## ğŸ“‹ å®Œæ•´ç¤ºä¾‹ï¼šä»é›¶å¼€å§‹

### æ­¥éª¤1ï¼šè®¾ç½®ä»£ç†ï¼ˆé€‰æ‹©ä¸€ç§æ–¹å¼ï¼‰

**æ–¹å¼Aï¼šä½¿ç”¨å†…ç½®ä»£ç†ï¼ˆæœ€ç®€å•ï¼‰**
```bash
# åªéœ€åŠ  --proxy å‚æ•°ï¼Œæ— éœ€å…¶ä»–è®¾ç½®
```

**æ–¹å¼Bï¼šä½¿ç”¨Clashç­‰ä»£ç†è½¯ä»¶**
```bash
# 1. ç¡®ä¿ä»£ç†è½¯ä»¶è¿è¡Œä¸­
# 2. æŸ¥çœ‹ä»£ç†ç«¯å£ï¼ˆé€šå¸¸æ˜¯7890ï¼‰
# 3. è®¾ç½®ç¯å¢ƒå˜é‡
export HTTP_PROXY=http://127.0.0.1:7890
export HTTPS_PROXY=http://127.0.0.1:7890
```

### æ­¥éª¤2ï¼šæµ‹è¯•ç½‘ç»œ

```bash
# æµ‹è¯•ä»£ç†æ˜¯å¦å·¥ä½œ
curl -x http://127.0.0.1:7890 https://www.google.com

# æˆ–è€…æµ‹è¯•Python
python -c "import requests; print(requests.get('https://scholar.google.com', proxies={'http': 'http://127.0.0.1:7890', 'https': 'http://127.0.0.1:7890'}).status_code)"
```

### æ­¥éª¤3ï¼šè¿è¡Œçˆ¬è™«ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰

```bash
# åŸºç¡€æœç´¢ + ä»£ç†
python scholar_crawler.py "deep learning computer vision" \
  --max 20 \
  --proxy

# å¦‚æœæˆåŠŸï¼Œå†æ·»åŠ æ¡ä»¶
python scholar_crawler.py "deep learning computer vision" \
  --year-start 2020 \
  --min-citations 10 \
  --max 30 \
  --proxy
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: ä½¿ç”¨ --proxy åä»ç„¶å¤±è´¥

**å¯èƒ½åŸå› **ï¼š
- å…è´¹ä»£ç†ä¸ç¨³å®š
- Google Scholar å°ç¦äº†ä»£ç†IP
- ç½‘ç»œé—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨è‡ªå·±çš„ä»£ç†ï¼ˆClash/V2Rayï¼‰
2. å‡å°‘è¯·æ±‚é¢‘ç‡ï¼ˆ`--max 20`ï¼‰
3. å¤šè¯•å‡ æ¬¡

### Q2: æ‰€æœ‰æ–‡çŒ®éƒ½è¢«ç­›æ‰

**åŸå› **ï¼šç­›é€‰æ¡ä»¶å¤ªä¸¥æ ¼

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. é™ä½ `citations_min`ï¼ˆä»50é™åˆ°10ï¼‰
2. ç§»é™¤ `venues` é™åˆ¶
3. æ¸…ç©º `exclude_keywords`
4. æ‰©å¤§å¹´ä»½èŒƒå›´

### Q3: ä»£ç†è®¾ç½®åæ— æ•ˆ

**æ£€æŸ¥æ¸…å•**ï¼š
```bash
# 1. æ£€æŸ¥ä»£ç†è½¯ä»¶æ˜¯å¦è¿è¡Œ
ps aux | grep -i clash  # æˆ–ä½ çš„ä»£ç†è½¯ä»¶å

# 2. æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®
netstat -an | grep 7890  # æˆ–ä½ çš„ä»£ç†ç«¯å£

# 3. æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $HTTP_PROXY
echo $HTTPS_PROXY

# 4. æµ‹è¯•ä»£ç†è¿æ¥
curl -x http://127.0.0.1:7890 https://www.google.com
```

### Q4: urllib3 è­¦å‘Šä¿¡æ¯

```
NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+
```

**è¿™æ˜¯è­¦å‘Šï¼Œä¸æ˜¯é”™è¯¯**ï¼Œä¸å½±å“ä½¿ç”¨ã€‚å¦‚æœæƒ³æ¶ˆé™¤ï¼š

```bash
pip install urllib3==1.26.15
```

---

## ğŸ¯ é’ˆå¯¹ä½ çš„æƒ…å†µçš„å…·ä½“å»ºè®®

åŸºäºä½ çš„é”™è¯¯ä¿¡æ¯ï¼Œæˆ‘å»ºè®®ï¼š

### æ–¹æ¡ˆAï¼šæ”¾å®½æ¡ä»¶ + å†…ç½®ä»£ç†ï¼ˆæ¨èï¼‰

```bash
python scholar_crawler.py "deep learning computer vision" \
  --year-start 2020 \
  --min-citations 10 \
  --max 30 \
  --proxy
```

### æ–¹æ¡ˆBï¼šä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼ˆå¦‚æœæœ‰Clashç­‰ï¼‰

```bash
# 1. ç¡®ä¿Clashè¿è¡Œä¸­ï¼Œç«¯å£é€šå¸¸æ˜¯7890
# 2. è®¾ç½®ç¯å¢ƒå˜é‡
export HTTP_PROXY=http://127.0.0.1:7890
export HTTPS_PROXY=http://127.0.0.1:7890

# 3. è¿è¡Œï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
python scholar_crawler.py "deep learning computer vision" \
  --year-start 2020 \
  --min-citations 10 \
  --max 30
```

### æ–¹æ¡ˆCï¼šåˆ†æ­¥æµ‹è¯•

```bash
# æ­¥éª¤1ï¼šæœ€ç®€å•çš„æœç´¢
python scholar_crawler.py "deep learning" --max 10 --proxy

# æ­¥éª¤2ï¼šå¦‚æœæˆåŠŸï¼Œæ·»åŠ å¹´ä»½
python scholar_crawler.py "deep learning" --year-start 2020 --max 10 --proxy

# æ­¥éª¤3ï¼šé€æ­¥æ·»åŠ å…¶ä»–æ¡ä»¶
python scholar_crawler.py "deep learning" \
  --year-start 2020 \
  --min-citations 10 \
  --max 20 \
  --proxy
```

---

## ğŸ“ æ¨èçš„é…ç½®æ–‡ä»¶

åˆ›å»º `configs/cv_relaxed.json`ï¼š

```json
{
  "description": "CVç›¸å…³æ–‡çŒ®ï¼ˆæ”¾å®½ç‰ˆï¼‰",
  "year_start": 2020,
  "year_end": null,
  "citations_min": 10,
  "citations_max": null,
  "authors": [],
  "publishers": [],
  "venues": [],
  "additional_keywords": [],
  "exclude_keywords": [],
  "keyword_mode": "OR"
}
```

ä½¿ç”¨ï¼š

```bash
python scholar_crawler.py "deep learning computer vision" \
  --config configs/cv_relaxed.json \
  --proxy
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **æ€»æ˜¯ä½¿ç”¨ä»£ç†**ï¼šæ·»åŠ  `--proxy` å‚æ•°
2. **ä»å®½åˆ°çª„**ï¼šå…ˆæ”¾å®½æ¡ä»¶ï¼Œçœ‹åˆ°ç»“æœåå†æ”¶ç´§
3. **æ§åˆ¶æ•°é‡**ï¼š`--max 20-30` å³å¯ï¼Œä¸è¦å¤ªå¤š
4. **è€å¿ƒç­‰å¾…**ï¼šæ¯ç¯‡æ–‡çŒ®ä¹‹é—´æœ‰2ç§’å»¶è¿Ÿ
5. **å¤šæ¬¡å°è¯•**ï¼šç½‘ç»œé—®é¢˜å¯èƒ½å¯¼è‡´å¶å°”å¤±è´¥

---

## ğŸ” è°ƒè¯•æŠ€å·§

```bash
# 1. æµ‹è¯•æœ€ç®€å•çš„æœç´¢
python scholar_crawler.py "deep learning" --max 5

# 2. å¦‚æœå¤±è´¥ï¼ŒåŠ ä»£ç†
python scholar_crawler.py "deep learning" --max 5 --proxy

# 3. å¦‚æœè¿˜å¤±è´¥ï¼Œæ£€æŸ¥ç½‘ç»œ
ping scholar.google.com

# 4. æ£€æŸ¥æ˜¯å¦è¢«é™åˆ¶
curl https://scholar.google.com
```

---

å¸Œæœ›è¿™èƒ½å¸®åˆ°ä½ ï¼å¦‚æœè¿˜æœ‰é—®é¢˜ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“çš„é”™è¯¯ä¿¡æ¯ã€‚ğŸš€

