#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨ç¤ºä¾‹è„šæœ¬
æ¼”ç¤ºå¦‚ä½•åœ¨ä»£ç ä¸­ä½¿ç”¨ ScholarCrawler
"""

from scholar_crawler import ScholarCrawler


def example_basic_search():
    """ç¤ºä¾‹1: åŸºç¡€æœç´¢"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹1: åŸºç¡€æœç´¢ - æœç´¢'machine learning'")
    print("="*60)
    
    crawler = ScholarCrawler(use_proxy=False)
    papers = crawler.search_papers("machine learning", max_results=10)
    
    if papers:
        # æŒ‰å¼•ç”¨é‡æ’åº
        papers = crawler.sort_by_citations(papers)
        
        # å¯¼å‡ºCSV
        crawler.export_to_csv(papers, "results/example1_basic.csv", "machine learning")
    

def example_with_filter():
    """ç¤ºä¾‹2: å¸¦ç­›é€‰çš„æœç´¢"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹2: é«˜å¼•ç”¨æ–‡çŒ®æœç´¢ - æœç´¢'deep learning'å¹¶ç­›é€‰å¼•ç”¨é‡>=100")
    print("="*60)
    
    crawler = ScholarCrawler(use_proxy=False)
    papers = crawler.search_papers("deep learning", max_results=30)
    
    if papers:
        # æŒ‰å¼•ç”¨é‡æ’åº
        papers = crawler.sort_by_citations(papers)
        
        # ç­›é€‰é«˜å¼•ç”¨æ–‡çŒ®
        papers = crawler.filter_by_citations(papers, min_citations=100)
        
        if papers:
            # å¯¼å‡ºCSV
            crawler.export_to_csv(papers, "results/example2_filtered.csv", "deep learning")
        else:
            print("âš  æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ®")


def example_with_proxy():
    """ç¤ºä¾‹3: ä½¿ç”¨ä»£ç†"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹3: ä½¿ç”¨ä»£ç†æœç´¢ - æœç´¢'computer vision'")
    print("="*60)
    
    crawler = ScholarCrawler(use_proxy=True)  # å¯ç”¨ä»£ç†
    papers = crawler.search_papers("computer vision", max_results=20)
    
    if papers:
        # æŒ‰å¼•ç”¨é‡æ’åº
        papers = crawler.sort_by_citations(papers)
        
        # å¯¼å‡ºCSV
        crawler.export_to_csv(papers, "results/example3_proxy.csv", "computer vision")


def example_multiple_keywords():
    """ç¤ºä¾‹4: æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®å­—"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹4: æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®å­—")
    print("="*60)
    
    keywords = [
        "neural networks",
        "reinforcement learning",
        "natural language processing"
    ]
    
    crawler = ScholarCrawler(use_proxy=False)
    
    for keyword in keywords:
        print(f"\n>>> æ­£åœ¨å¤„ç†å…³é”®å­—: {keyword}")
        papers = crawler.search_papers(keyword, max_results=15)
        
        if papers:
            papers = crawler.sort_by_citations(papers)
            safe_keyword = keyword.replace(' ', '_')
            crawler.export_to_csv(
                papers, 
                f"results/example4_{safe_keyword}.csv", 
                keyword
            )
        
        # åœ¨å…³é”®å­—ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
        print("ç­‰å¾…5ç§’åç»§ç»­...")
        import time
        time.sleep(5)


def example_custom_analysis():
    """ç¤ºä¾‹5: è‡ªå®šä¹‰åˆ†æ"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹5: è‡ªå®šä¹‰åˆ†æ - åˆ†æä¸åŒå¹´ä»½çš„æ–‡çŒ®åˆ†å¸ƒ")
    print("="*60)
    
    crawler = ScholarCrawler(use_proxy=False)
    papers = crawler.search_papers("artificial intelligence", max_results=50)
    
    if papers:
        # æŒ‰å¹´ä»½åˆ†ç»„ç»Ÿè®¡
        year_stats = {}
        for paper in papers:
            year = paper['year']
            if year != 'N/A':
                year_stats[year] = year_stats.get(year, 0) + 1
        
        print("\nğŸ“Š å¹´ä»½åˆ†å¸ƒ:")
        for year in sorted(year_stats.keys(), reverse=True):
            print(f"  {year}: {year_stats[year]} ç¯‡")
        
        # è®¡ç®—å¹³å‡å¼•ç”¨æ•° by year
        year_citations = {}
        for paper in papers:
            year = paper['year']
            if year != 'N/A':
                if year not in year_citations:
                    year_citations[year] = []
                year_citations[year].append(paper['citations'])
        
        print("\nğŸ“ˆ å¹´ä»½å¹³å‡å¼•ç”¨æ•°:")
        for year in sorted(year_citations.keys(), reverse=True):
            avg = sum(year_citations[year]) / len(year_citations[year])
            print(f"  {year}: {avg:.1f} æ¬¡")
        
        # å¯¼å‡ºç»“æœ
        papers = crawler.sort_by_citations(papers)
        crawler.export_to_csv(papers, "results/example5_analysis.csv", "artificial intelligence")


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…· - ä½¿ç”¨ç¤ºä¾‹             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

è¯·é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹:
  1. åŸºç¡€æœç´¢
  2. å¸¦ç­›é€‰çš„æœç´¢ (é«˜å¼•ç”¨æ–‡çŒ®)
  3. ä½¿ç”¨ä»£ç†æœç´¢
  4. æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®å­—
  5. è‡ªå®šä¹‰åˆ†æ
  0. è¿è¡Œæ‰€æœ‰ç¤ºä¾‹

è¾“å…¥é€‰é¡¹ (0-5): """, end='')
    
    choice = input().strip()
    
    examples = {
        '1': example_basic_search,
        '2': example_with_filter,
        '3': example_with_proxy,
        '4': example_multiple_keywords,
        '5': example_custom_analysis,
    }
    
    if choice == '0':
        print("\n>>> è¿è¡Œæ‰€æœ‰ç¤ºä¾‹...")
        for func in examples.values():
            func()
            print("\n" + "-"*60)
    elif choice in examples:
        examples[choice]()
    else:
        print("âŒ æ— æ•ˆçš„é€‰é¡¹")
    
    print("\nâœ“ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")


if __name__ == '__main__':
    main()

