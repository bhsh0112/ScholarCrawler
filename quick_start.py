#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¿«é€Ÿå¼€å§‹è„šæœ¬
äº¤äº’å¼å¼•å¯¼ç”¨æˆ·ä½¿ç”¨ Scholar Crawler
"""

from scholar_crawler import ScholarCrawler
import config


def quick_start():
    """å¿«é€Ÿå¼€å§‹å‘å¯¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     æ¬¢è¿ä½¿ç”¨ Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…· ğŸ“              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # 1. é€‰æ‹©å…³é”®å­—æ¥æº
    print("ç¬¬ä¸€æ­¥: é€‰æ‹©æœç´¢å…³é”®å­—æ¥æº")
    print("  1. æ‰‹åŠ¨è¾“å…¥å…³é”®å­—")
    print("  2. ä»é¢„è®¾åˆ—è¡¨é€‰æ‹© (AI/CV/NLP/æ¨èç³»ç»Ÿ/æ•°æ®æŒ–æ˜/å¤§æ•°æ®)")
    
    choice = input("\nè¯·é€‰æ‹© (1-2): ").strip()
    
    keyword = ""
    if choice == '1':
        keyword = input("\nè¯·è¾“å…¥æœç´¢å…³é”®å­—: ").strip()
    elif choice == '2':
        print("\né¢„è®¾ç±»åˆ«:")
        print("  1. äººå·¥æ™ºèƒ½ (AI)")
        print("  2. è®¡ç®—æœºè§†è§‰ (CV)")
        print("  3. è‡ªç„¶è¯­è¨€å¤„ç† (NLP)")
        print("  4. æ¨èç³»ç»Ÿ (RecSys)")
        print("  5. æ•°æ®æŒ–æ˜ (DM)")
        print("  6. å¤§æ•°æ® (BigData)")
        
        cat_choice = input("\nè¯·é€‰æ‹©ç±»åˆ« (1-6): ").strip()
        cat_map = {
            '1': 'ai', '2': 'cv', '3': 'nlp',
            '4': 'recsys', '5': 'dm', '6': 'bigdata'
        }
        
        if cat_choice in cat_map:
            keywords = config.get_keywords_by_category(cat_map[cat_choice])
            print(f"\nè¯¥ç±»åˆ«åŒ…å«ä»¥ä¸‹å…³é”®å­—:")
            for i, kw in enumerate(keywords, 1):
                print(f"  {i}. {kw}")
            
            kw_choice = input(f"\nè¯·é€‰æ‹©å…³é”®å­— (1-{len(keywords)}): ").strip()
            try:
                keyword = keywords[int(kw_choice) - 1]
            except:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®å­—")
                keyword = keywords[0]
    
    if not keyword:
        print("âŒ æœªé€‰æ‹©å…³é”®å­—ï¼Œé€€å‡º")
        return
    
    print(f"\nâœ“ å·²é€‰æ‹©å…³é”®å­—: '{keyword}'")
    
    # 2. è®¾ç½®å‚æ•°
    print("\n" + "-"*60)
    print("ç¬¬äºŒæ­¥: è®¾ç½®æœç´¢å‚æ•°")
    
    max_results = input(f"æœ€å¤§è·å–æ–‡çŒ®æ•°é‡ (é»˜è®¤: {config.DEFAULT_MAX_RESULTS}): ").strip()
    max_results = int(max_results) if max_results.isdigit() else config.DEFAULT_MAX_RESULTS
    
    use_proxy = input("æ˜¯å¦ä½¿ç”¨ä»£ç†? (y/n, é»˜è®¤: n): ").strip().lower() == 'y'
    
    output_file = input("è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ): ").strip()
    if not output_file:
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_keyword = keyword.replace(' ', '_').replace('/', '_')
        output_file = f"{config.OUTPUT_DIR}/{safe_keyword}_{timestamp}.csv"
    
    # 2.5 é«˜çº§æ£€ç´¢é€‰é¡¹
    print("\n" + "-"*60)
    print("ç¬¬ä¸‰æ­¥: é«˜çº§æ£€ç´¢é€‰é¡¹ï¼ˆå¯é€‰ï¼‰")
    print("  1. ä½¿ç”¨é¢„è®¾é…ç½®ï¼ˆæ¨èï¼‰")
    print("  2. è‡ªå®šä¹‰é«˜çº§æ£€ç´¢")
    print("  3. è·³è¿‡ï¼ˆä½¿ç”¨åŸºç¡€æœç´¢ï¼‰")
    
    advanced_choice = input("\nè¯·é€‰æ‹© (1-3, é»˜è®¤: 3): ").strip() or '3'
    
    advanced_config = None
    
    if advanced_choice == '1':
        # ä½¿ç”¨é¢„è®¾é…ç½®
        print("\né¢„è®¾é…ç½®:")
        print("  1. æœ€è¿‘é«˜å½±å“åŠ›æ–‡çŒ® (2020å¹´å, å¼•ç”¨>=50)")
        print("  2. é¡¶çº§AIä¼šè®® (NeurIPS, ICML, CVPRç­‰)")
        print("  3. Nature/Science é¡¶çº§æœŸåˆŠ")
        
        preset_choice = input("\né€‰æ‹©é¢„è®¾é…ç½® (1-3): ").strip()
        
        if preset_choice == '1':
            advanced_config = config.create_recent_high_impact_config(2020, 50)
        elif preset_choice == '2':
            advanced_config = config.create_top_venue_config(config.TOP_AI_CONFERENCES)
        elif preset_choice == '3':
            advanced_config = config.create_top_venue_config(config.TOP_AI_JOURNALS)
        
        if advanced_config:
            print(f"\nâœ“ å·²é€‰æ‹©é¢„è®¾é…ç½®")
    
    elif advanced_choice == '2':
        # è‡ªå®šä¹‰é«˜çº§æ£€ç´¢
        advanced_config = config.AdvancedSearchConfig()
        
        print("\nè‡ªå®šä¹‰é«˜çº§æ£€ç´¢å‚æ•°ï¼ˆç›´æ¥å›è½¦è·³è¿‡ï¼‰:")
        
        # å¹´ä»½èŒƒå›´
        year_start = input("  èµ·å§‹å¹´ä»½ (å¦‚: 2020): ").strip()
        if year_start.isdigit():
            advanced_config.year_start = int(year_start)
        
        year_end = input("  ç»“æŸå¹´ä»½ (å¦‚: 2023): ").strip()
        if year_end.isdigit():
            advanced_config.year_end = int(year_end)
        
        # å¼•ç”¨é‡
        min_cit = input("  æœ€å°å¼•ç”¨é‡ (å¦‚: 50): ").strip()
        if min_cit.isdigit():
            advanced_config.citations_min = int(min_cit)
        
        max_cit = input("  æœ€å¤§å¼•ç”¨é‡ (ç•™ç©ºä¸é™): ").strip()
        if max_cit.isdigit():
            advanced_config.citations_max = int(max_cit)
        
        # ä½œè€…
        authors = input("  ä½œè€… (å¤šä¸ªç”¨é€—å·åˆ†éš”): ").strip()
        if authors:
            advanced_config.authors = [a.strip() for a in authors.split(',')]
        
        # ä¼šè®®/æœŸåˆŠ
        venues = input("  ä¼šè®®/æœŸåˆŠ (å¦‚: CVPR,NeurIPS): ").strip()
        if venues:
            advanced_config.venues = [v.strip() for v in venues.split(',')]
        
        # å‡ºç‰ˆå•†
        publishers = input("  å‡ºç‰ˆå•† (å¦‚: IEEE,ACM): ").strip()
        if publishers:
            advanced_config.publishers = [p.strip() for p in publishers.split(',')]
        
        # æ’é™¤è¯
        exclude = input("  æ’é™¤å…³é”®å­— (å¦‚: survey,review): ").strip()
        if exclude:
            advanced_config.exclude_keywords = [e.strip() for e in exclude.split(',')]
        
        print("\nâœ“ å·²é…ç½®é«˜çº§æ£€ç´¢")
    
    # 3. ç¡®è®¤ä¿¡æ¯
    print("\n" + "="*60)
    print("æœç´¢é…ç½®ç¡®è®¤:")
    print("="*60)
    print(f"  å…³é”®å­—: {keyword}")
    print(f"  æœ€å¤§æ•°é‡: {max_results}")
    print(f"  ä½¿ç”¨ä»£ç†: {'æ˜¯' if use_proxy else 'å¦'}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    if advanced_config:
        print("\n  é«˜çº§æ£€ç´¢:")
        print(advanced_config)
    
    print("="*60)
    
    confirm = input("\nç¡®è®¤å¼€å§‹æœç´¢? (y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ å·²å–æ¶ˆ")
        return
    
    # 4. æ‰§è¡Œæœç´¢
    print("\n" + "="*60)
    print("å¼€å§‹æœç´¢...")
    print("="*60)
    
    try:
        crawler = ScholarCrawler(use_proxy=use_proxy)
        papers = crawler.search_papers(keyword, max_results=max_results, 
                                       advanced_config=advanced_config)
        
        if not papers:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ–‡çŒ®")
            return
        
        # æ’åº
        papers = crawler.sort_by_citations(papers)
        
        # å¯¼å‡º
        crawler.export_to_csv(papers, output_file, keyword)
        
        print("\n" + "="*60)
        print("âœ“ æœç´¢å®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print("\næ‚¨å¯ä»¥ä½¿ç”¨ Excel æˆ–å…¶ä»–å·¥å…·æ‰“å¼€ CSV æ–‡ä»¶æŸ¥çœ‹ç»“æœ")
        
    except KeyboardInterrupt:
        print("\n\nâš  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨ä»£ç†")


def batch_search():
    """æ‰¹é‡æœç´¢æ¨¡å¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            æ‰¹é‡æœç´¢æ¨¡å¼ ğŸ“š                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    print("è¯·é€‰æ‹©æ‰¹é‡æœç´¢ç±»åˆ«:")
    print("  1. äººå·¥æ™ºèƒ½ (6ä¸ªå…³é”®å­—)")
    print("  2. è®¡ç®—æœºè§†è§‰ (6ä¸ªå…³é”®å­—)")
    print("  3. è‡ªç„¶è¯­è¨€å¤„ç† (6ä¸ªå…³é”®å­—)")
    print("  4. æ¨èç³»ç»Ÿ (5ä¸ªå…³é”®å­—)")
    print("  5. æ•°æ®æŒ–æ˜ (6ä¸ªå…³é”®å­—)")
    print("  6. å¤§æ•°æ® (5ä¸ªå…³é”®å­—)")
    print("  7. å…¨éƒ¨ (æ‰€æœ‰é¢„è®¾å…³é”®å­—)")
    
    choice = input("\nè¯·é€‰æ‹© (1-7): ").strip()
    
    cat_map = {
        '1': ('ai', config.AI_KEYWORDS),
        '2': ('cv', config.CV_KEYWORDS),
        '3': ('nlp', config.NLP_KEYWORDS),
        '4': ('recsys', config.RECSYS_KEYWORDS),
        '5': ('dm', config.DM_KEYWORDS),
        '6': ('bigdata', config.BIGDATA_KEYWORDS),
        '7': ('all', config.get_all_keywords()),
    }
    
    if choice not in cat_map:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    cat_name, keywords = cat_map[choice]
    
    print(f"\nå°†æœç´¢ä»¥ä¸‹ {len(keywords)} ä¸ªå…³é”®å­—:")
    for i, kw in enumerate(keywords, 1):
        print(f"  {i}. {kw}")
    
    max_results = input(f"\næ¯ä¸ªå…³é”®å­—çš„æœ€å¤§æ–‡çŒ®æ•° (é»˜è®¤: 20): ").strip()
    max_results = int(max_results) if max_results.isdigit() else 20
    
    use_proxy = input("æ˜¯å¦ä½¿ç”¨ä»£ç†? (y/n, é»˜è®¤: n): ").strip().lower() == 'y'
    
    confirm = input(f"\nç¡®è®¤æ‰¹é‡æœç´¢ {len(keywords)} ä¸ªå…³é”®å­—? (y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ å·²å–æ¶ˆ")
        return
    
    # æ‰§è¡Œæ‰¹é‡æœç´¢
    print("\n" + "="*60)
    print("å¼€å§‹æ‰¹é‡æœç´¢...")
    print("="*60)
    
    crawler = ScholarCrawler(use_proxy=use_proxy)
    
    from datetime import datetime
    import time
    
    for i, keyword in enumerate(keywords, 1):
        print(f"\n[{i}/{len(keywords)}] æœç´¢: {keyword}")
        print("-"*60)
        
        try:
            papers = crawler.search_papers(keyword, max_results=max_results)
            
            if papers:
                papers = crawler.sort_by_citations(papers)
                
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                safe_keyword = keyword.replace(' ', '_').replace('/', '_')
                output_file = f"{config.OUTPUT_DIR}/{cat_name}_{safe_keyword}_{timestamp}.csv"
                
                crawler.export_to_csv(papers, output_file, keyword)
            
            # åœ¨å…³é”®å­—ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
            if i < len(keywords):
                print("\nâ³ ç­‰å¾…5ç§’åç»§ç»­...")
                time.sleep(5)
        
        except KeyboardInterrupt:
            print("\n\nâš  ç”¨æˆ·ä¸­æ–­æ‰¹é‡æœç´¢")
            break
        except Exception as e:
            print(f"âŒ æœç´¢ '{keyword}' æ—¶å‡ºé”™: {e}")
            continue
    
    print("\n" + "="*60)
    print("âœ“ æ‰¹é‡æœç´¢å®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {config.OUTPUT_DIR}/ ç›®å½•")


def advanced_search_wizard():
    """é«˜çº§æ£€ç´¢å‘å¯¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            é«˜çº§æ£€ç´¢å‘å¯¼ ğŸ”¬                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½
    print("ä½¿ç”¨é¢„è®¾é…ç½®æ–‡ä»¶æˆ–è‡ªå®šä¹‰ï¼Ÿ")
    print("  1. åŠ è½½é¢„è®¾é…ç½®æ–‡ä»¶")
    print("  2. è‡ªå®šä¹‰é…ç½®ï¼ˆä¿å­˜ä¸ºæ–‡ä»¶ï¼‰")
    
    choice = input("\nè¯·é€‰æ‹© (1-2): ").strip()
    
    if choice == '1':
        import os
        config_dir = "configs"
        if os.path.exists(config_dir):
            import glob
            json_files = glob.glob(f"{config_dir}/*.json")
            if json_files:
                print("\nå¯ç”¨çš„é…ç½®æ–‡ä»¶:")
                for i, f in enumerate(json_files, 1):
                    print(f"  {i}. {os.path.basename(f)}")
                
                file_choice = input(f"\né€‰æ‹©é…ç½®æ–‡ä»¶ (1-{len(json_files)}): ").strip()
                try:
                    config_file = json_files[int(file_choice) - 1]
                    keyword = input("\nè¯·è¾“å…¥æœç´¢å…³é”®å­—: ").strip()
                    
                    # è°ƒç”¨å‘½ä»¤è¡Œå·¥å…·
                    import subprocess
                    cmd = f'python scholar_crawler.py "{keyword}" --config "{config_file}"'
                    print(f"\næ‰§è¡Œå‘½ä»¤: {cmd}\n")
                    subprocess.call(cmd, shell=True)
                    return
                except:
                    print("âŒ æ— æ•ˆé€‰æ‹©")
                    return
        
        print("âš  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶")
    
    elif choice == '2':
        print("\nè‡ªå®šä¹‰é«˜çº§æ£€ç´¢é…ç½®")
        print("è¯·æŒ‰æç¤ºè¾“å…¥å‚æ•°ï¼ˆç›´æ¥å›è½¦è·³è¿‡ï¼‰\n")
        
        config_dict = {}
        
        keyword = input("æœç´¢å…³é”®å­—: ").strip()
        if not keyword:
            print("âŒ å¿…é¡»æä¾›å…³é”®å­—")
            return
        
        # æ”¶é›†å‚æ•°
        year_start = input("èµ·å§‹å¹´ä»½ (å¦‚: 2020): ").strip()
        if year_start.isdigit():
            config_dict['year_start'] = int(year_start)
        
        year_end = input("ç»“æŸå¹´ä»½ (å¦‚: 2023): ").strip()
        if year_end.isdigit():
            config_dict['year_end'] = int(year_end)
        
        min_cit = input("æœ€å°å¼•ç”¨é‡: ").strip()
        if min_cit.isdigit():
            config_dict['citations_min'] = int(min_cit)
        
        authors = input("ä½œè€… (é€—å·åˆ†éš”): ").strip()
        if authors:
            config_dict['authors'] = [a.strip() for a in authors.split(',')]
        
        venues = input("ä¼šè®®/æœŸåˆŠ (é€—å·åˆ†éš”): ").strip()
        if venues:
            config_dict['venues'] = [v.strip() for v in venues.split(',')]
        
        # ä¿å­˜é…ç½®
        save = input("\næ˜¯å¦ä¿å­˜é…ç½®åˆ°æ–‡ä»¶? (y/n): ").strip().lower()
        if save == 'y':
            import json
            from datetime import datetime
            config_name = input("é…ç½®æ–‡ä»¶å (æ— éœ€åç¼€): ").strip() or f"custom_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            config_file = f"configs/{config_name}.json"
            
            import os
            os.makedirs("configs", exist_ok=True)
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, ensure_ascii=False, indent=2)
            
            print(f"âœ“ é…ç½®å·²ä¿å­˜: {config_file}")
            
            # ä½¿ç”¨é…ç½®æœç´¢
            execute = input("æ˜¯å¦ç«‹å³ä½¿ç”¨æ­¤é…ç½®æœç´¢? (y/n): ").strip().lower()
            if execute == 'y':
                import subprocess
                cmd = f'python scholar_crawler.py "{keyword}" --config "{config_file}"'
                print(f"\næ‰§è¡Œå‘½ä»¤: {cmd}\n")
                subprocess.call(cmd, shell=True)


def main():
    """ä¸»èœå•"""
    while True:
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…· - å¿«é€Ÿå¼€å§‹              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

è¯·é€‰æ‹©æ¨¡å¼:
  1. å¿«é€Ÿå¼€å§‹ (å•ä¸ªå…³é”®å­—æœç´¢)
  2. æ‰¹é‡æœç´¢ (å¤šä¸ªå…³é”®å­—)
  3. é«˜çº§æ£€ç´¢å‘å¯¼ (é…ç½®æ–‡ä»¶ç®¡ç†)
  4. æŸ¥çœ‹é…ç½®
  5. é€€å‡º

""")
        
        choice = input("è¯·é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            quick_start()
        elif choice == '2':
            batch_search()
        elif choice == '3':
            advanced_search_wizard()
        elif choice == '4':
            print(f"\nå½“å‰é…ç½®:")
            print(f"  é»˜è®¤æœç´¢æ•°é‡: {config.DEFAULT_MAX_RESULTS}")
            print(f"  é»˜è®¤æœ€å°å¼•ç”¨: {config.DEFAULT_MIN_CITATIONS}")
            print(f"  è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
            print(f"  é¢„è®¾å…³é”®å­—æ€»æ•°: {len(config.get_all_keywords())}")
            print(f"  é¡¶çº§AIä¼šè®®: {len(config.TOP_AI_CONFERENCES)} ä¸ª")
            print(f"  é¡¶çº§AIæœŸåˆŠ: {len(config.TOP_AI_JOURNALS)} ä¸ª")
            input("\næŒ‰å›è½¦ç»§ç»­...")
        elif choice == '5':
            print("\nå†è§! ğŸ‘‹")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©\n")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nå†è§! ğŸ‘‹")

