#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¿«é€Ÿå¼€å§‹è„šæœ¬
äº¤äº’å¼å¼•å¯¼ç”¨æˆ·ä½¿ç”¨ Scholar Crawler
"""

from scholar_crawler import ScholarCrawler
import config


def quick_start():
    """å¿«é€Ÿå¼€å§‹å‘å¯¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     æ¬¢è¿ä½¿ç”¨ Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…· ğŸ“              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # 1. é€‰æ‹©å…³é”®å­—æ¥æº
    print("ç¬¬ä¸€æ­¥: é€‰æ‹©æœç´¢å…³é”®å­—æ¥æº")
    print("  1. æ‰‹åŠ¨è¾“å…¥å…³é”®å­—")
    print("  2. ä»é¢„è®¾åˆ—è¡¨é€‰æ‹© (AI/CV/NLP/æ¨èç³»ç»Ÿ/æ•°æ®æŒ–æ˜/å¤§æ•°æ®)")
    
    choice = input("\nè¯·é€‰æ‹© (1-2): ").strip()
    
    keyword = ""
    if choice == '1':
        keyword = input("\nè¯·è¾“å…¥æœç´¢å…³é”®å­—: ").strip()
    elif choice == '2':
        print("\né¢„è®¾ç±»åˆ«:")
        print("  1. äººå·¥æ™ºèƒ½ (AI)")
        print("  2. è®¡ç®—æœºè§†è§‰ (CV)")
        print("  3. è‡ªç„¶è¯­è¨€å¤„ç† (NLP)")
        print("  4. æ¨èç³»ç»Ÿ (RecSys)")
        print("  5. æ•°æ®æŒ–æ˜ (DM)")
        print("  6. å¤§æ•°æ® (BigData)")
        
        cat_choice = input("\nè¯·é€‰æ‹©ç±»åˆ« (1-6): ").strip()
        cat_map = {
            '1': 'ai', '2': 'cv', '3': 'nlp',
            '4': 'recsys', '5': 'dm', '6': 'bigdata'
        }
        
        if cat_choice in cat_map:
            keywords = config.get_keywords_by_category(cat_map[cat_choice])
            print(f"\nè¯¥ç±»åˆ«åŒ…å«ä»¥ä¸‹å…³é”®å­—:")
            for i, kw in enumerate(keywords, 1):
                print(f"  {i}. {kw}")
            
            kw_choice = input(f"\nè¯·é€‰æ‹©å…³é”®å­— (1-{len(keywords)}): ").strip()
            try:
                keyword = keywords[int(kw_choice) - 1]
            except:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®å­—")
                keyword = keywords[0]
    
    if not keyword:
        print("âŒ æœªé€‰æ‹©å…³é”®å­—ï¼Œé€€å‡º")
        return
    
    print(f"\nâœ“ å·²é€‰æ‹©å…³é”®å­—: '{keyword}'")
    
    # 2. è®¾ç½®å‚æ•°
    print("\n" + "-"*60)
    print("ç¬¬äºŒæ­¥: è®¾ç½®æœç´¢å‚æ•°")
    
    max_results = input(f"æœ€å¤§è·å–æ–‡çŒ®æ•°é‡ (é»˜è®¤: {config.DEFAULT_MAX_RESULTS}): ").strip()
    max_results = int(max_results) if max_results.isdigit() else config.DEFAULT_MAX_RESULTS
    
    min_citations = input(f"æœ€å°å¼•ç”¨é‡ç­›é€‰ (é»˜è®¤: {config.DEFAULT_MIN_CITATIONS}): ").strip()
    min_citations = int(min_citations) if min_citations.isdigit() else config.DEFAULT_MIN_CITATIONS
    
    use_proxy = input("æ˜¯å¦ä½¿ç”¨ä»£ç†? (y/n, é»˜è®¤: n): ").strip().lower() == 'y'
    
    output_file = input("è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ): ").strip()
    if not output_file:
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_keyword = keyword.replace(' ', '_').replace('/', '_')
        output_file = f"{config.OUTPUT_DIR}/{safe_keyword}_{timestamp}.csv"
    
    # 3. ç¡®è®¤ä¿¡æ¯
    print("\n" + "="*60)
    print("æœç´¢é…ç½®ç¡®è®¤:")
    print("="*60)
    print(f"  å…³é”®å­—: {keyword}")
    print(f"  æœ€å¤§æ•°é‡: {max_results}")
    print(f"  æœ€å°å¼•ç”¨: {min_citations}")
    print(f"  ä½¿ç”¨ä»£ç†: {'æ˜¯' if use_proxy else 'å¦'}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("="*60)
    
    confirm = input("\nç¡®è®¤å¼€å§‹æœç´¢? (y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ å·²å–æ¶ˆ")
        return
    
    # 4. æ‰§è¡Œæœç´¢
    print("\n" + "="*60)
    print("å¼€å§‹æœç´¢...")
    print("="*60)
    
    try:
        crawler = ScholarCrawler(use_proxy=use_proxy)
        papers = crawler.search_papers(keyword, max_results=max_results)
        
        if not papers:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ–‡çŒ®")
            return
        
        # æ’åº
        papers = crawler.sort_by_citations(papers)
        
        # ç­›é€‰
        if min_citations > 0:
            papers = crawler.filter_by_citations(papers, min_citations)
            if not papers:
                print(f"âŒ æ²¡æœ‰å¼•ç”¨é‡ >= {min_citations} çš„æ–‡çŒ®")
                return
        
        # å¯¼å‡º
        crawler.export_to_csv(papers, output_file, keyword)
        
        print("\n" + "="*60)
        print("âœ“ æœç´¢å®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print("\næ‚¨å¯ä»¥ä½¿ç”¨ Excel æˆ–å…¶ä»–å·¥å…·æ‰“å¼€ CSV æ–‡ä»¶æŸ¥çœ‹ç»“æœ")
        
    except KeyboardInterrupt:
        print("\n\nâš  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨ä»£ç†")


def batch_search():
    """æ‰¹é‡æœç´¢æ¨¡å¼"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            æ‰¹é‡æœç´¢æ¨¡å¼ ğŸ“š                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    print("è¯·é€‰æ‹©æ‰¹é‡æœç´¢ç±»åˆ«:")
    print("  1. äººå·¥æ™ºèƒ½ (6ä¸ªå…³é”®å­—)")
    print("  2. è®¡ç®—æœºè§†è§‰ (6ä¸ªå…³é”®å­—)")
    print("  3. è‡ªç„¶è¯­è¨€å¤„ç† (6ä¸ªå…³é”®å­—)")
    print("  4. æ¨èç³»ç»Ÿ (5ä¸ªå…³é”®å­—)")
    print("  5. æ•°æ®æŒ–æ˜ (6ä¸ªå…³é”®å­—)")
    print("  6. å¤§æ•°æ® (5ä¸ªå…³é”®å­—)")
    print("  7. å…¨éƒ¨ (æ‰€æœ‰é¢„è®¾å…³é”®å­—)")
    
    choice = input("\nè¯·é€‰æ‹© (1-7): ").strip()
    
    cat_map = {
        '1': ('ai', config.AI_KEYWORDS),
        '2': ('cv', config.CV_KEYWORDS),
        '3': ('nlp', config.NLP_KEYWORDS),
        '4': ('recsys', config.RECSYS_KEYWORDS),
        '5': ('dm', config.DM_KEYWORDS),
        '6': ('bigdata', config.BIGDATA_KEYWORDS),
        '7': ('all', config.get_all_keywords()),
    }
    
    if choice not in cat_map:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    cat_name, keywords = cat_map[choice]
    
    print(f"\nå°†æœç´¢ä»¥ä¸‹ {len(keywords)} ä¸ªå…³é”®å­—:")
    for i, kw in enumerate(keywords, 1):
        print(f"  {i}. {kw}")
    
    max_results = input(f"\næ¯ä¸ªå…³é”®å­—çš„æœ€å¤§æ–‡çŒ®æ•° (é»˜è®¤: 20): ").strip()
    max_results = int(max_results) if max_results.isdigit() else 20
    
    use_proxy = input("æ˜¯å¦ä½¿ç”¨ä»£ç†? (y/n, é»˜è®¤: n): ").strip().lower() == 'y'
    
    confirm = input(f"\nç¡®è®¤æ‰¹é‡æœç´¢ {len(keywords)} ä¸ªå…³é”®å­—? (y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ å·²å–æ¶ˆ")
        return
    
    # æ‰§è¡Œæ‰¹é‡æœç´¢
    print("\n" + "="*60)
    print("å¼€å§‹æ‰¹é‡æœç´¢...")
    print("="*60)
    
    crawler = ScholarCrawler(use_proxy=use_proxy)
    
    from datetime import datetime
    import time
    
    for i, keyword in enumerate(keywords, 1):
        print(f"\n[{i}/{len(keywords)}] æœç´¢: {keyword}")
        print("-"*60)
        
        try:
            papers = crawler.search_papers(keyword, max_results=max_results)
            
            if papers:
                papers = crawler.sort_by_citations(papers)
                
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                safe_keyword = keyword.replace(' ', '_').replace('/', '_')
                output_file = f"{config.OUTPUT_DIR}/{cat_name}_{safe_keyword}_{timestamp}.csv"
                
                crawler.export_to_csv(papers, output_file, keyword)
            
            # åœ¨å…³é”®å­—ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
            if i < len(keywords):
                print("\nâ³ ç­‰å¾…5ç§’åç»§ç»­...")
                time.sleep(5)
        
        except KeyboardInterrupt:
            print("\n\nâš  ç”¨æˆ·ä¸­æ–­æ‰¹é‡æœç´¢")
            break
        except Exception as e:
            print(f"âŒ æœç´¢ '{keyword}' æ—¶å‡ºé”™: {e}")
            continue
    
    print("\n" + "="*60)
    print("âœ“ æ‰¹é‡æœç´¢å®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {config.OUTPUT_DIR}/ ç›®å½•")


def main():
    """ä¸»èœå•"""
    while True:
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…· - å¿«é€Ÿå¼€å§‹              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

è¯·é€‰æ‹©æ¨¡å¼:
  1. å¿«é€Ÿå¼€å§‹ (å•ä¸ªå…³é”®å­—æœç´¢)
  2. æ‰¹é‡æœç´¢ (å¤šä¸ªå…³é”®å­—)
  3. æŸ¥çœ‹é…ç½®
  4. é€€å‡º

""")
        
        choice = input("è¯·é€‰æ‹© (1-4): ").strip()
        
        if choice == '1':
            quick_start()
        elif choice == '2':
            batch_search()
        elif choice == '3':
            print(f"\nå½“å‰é…ç½®:")
            print(f"  é»˜è®¤æœç´¢æ•°é‡: {config.DEFAULT_MAX_RESULTS}")
            print(f"  é»˜è®¤æœ€å°å¼•ç”¨: {config.DEFAULT_MIN_CITATIONS}")
            print(f"  è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
            print(f"  é¢„è®¾å…³é”®å­—æ€»æ•°: {len(config.get_all_keywords())}")
            input("\næŒ‰å›è½¦ç»§ç»­...")
        elif choice == '4':
            print("\nå†è§! ğŸ‘‹")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©\n")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nå†è§! ğŸ‘‹")

