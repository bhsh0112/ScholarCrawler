#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…·
åŠŸèƒ½ï¼šæ ¹æ®å…³é”®å­—æœç´¢æ–‡çŒ®ï¼ŒæŒ‰å¼•ç”¨é‡æŽ’åºï¼Œå¯¼å‡ºCSV
"""

import csv
import time
import argparse
from datetime import datetime
from typing import List, Dict
import os

try:
    from scholarly import scholarly, ProxyGenerator
except ImportError:
    print("è¯·å…ˆå®‰è£… scholarly åº“: pip install scholarly")
    exit(1)


class ScholarCrawler:
    """Google Scholar æ–‡çŒ®çˆ¬å–å™¨"""
    
    def __init__(self, use_proxy=False):
        """
        åˆå§‹åŒ–çˆ¬å–å™¨
        
        Args:
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†ï¼ˆæŽ¨èä½¿ç”¨ä»¥é¿å…è¢«å°ï¼‰
        """
        self.use_proxy = use_proxy
        if use_proxy:
            self._setup_proxy()
    
    def _setup_proxy(self):
        """è®¾ç½®ä»£ç†"""
        try:
            pg = ProxyGenerator()
            pg.FreeProxies()
            scholarly.use_proxy(pg)
            print("âœ“ ä»£ç†è®¾ç½®æˆåŠŸ")
        except Exception as e:
            print(f"âš  ä»£ç†è®¾ç½®å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨ç›´è¿žæ¨¡å¼ï¼Œå¯èƒ½ä¼šé‡åˆ°è®¿é—®é™åˆ¶")
    
    def search_papers(self, keyword: str, max_results: int = 50) -> List[Dict]:
        """
        æœç´¢æ–‡çŒ®
        
        Args:
            keyword: æœç´¢å…³é”®å­—
            max_results: æœ€å¤§ç»“æžœæ•°é‡
            
        Returns:
            æ–‡çŒ®åˆ—è¡¨
        """
        print(f"\nðŸ” æ­£åœ¨æœç´¢å…³é”®å­—: '{keyword}'")
        print(f"ðŸ“Š ç›®æ ‡èŽ·å–æ•°é‡: {max_results}")
        
        papers = []
        try:
            search_query = scholarly.search_pubs(keyword)
            
            for i, paper in enumerate(search_query):
                if i >= max_results:
                    break
                
                try:
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper_info = self._extract_paper_info(paper)
                    papers.append(paper_info)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if (i + 1) % 10 == 0:
                        print(f"  å·²èŽ·å– {i + 1} ç¯‡æ–‡çŒ®...")
                    
                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¢«å°
                    time.sleep(2)
                    
                except Exception as e:
                    print(f"  âš  å¤„ç†ç¬¬ {i + 1} ç¯‡æ–‡çŒ®æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"âœ“ æˆåŠŸèŽ·å– {len(papers)} ç¯‡æ–‡çŒ®\n")
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
        
        return papers
    
    def _extract_paper_info(self, paper) -> Dict:
        """
        æå–è®ºæ–‡ä¿¡æ¯
        
        Args:
            paper: scholarly è¿”å›žçš„è®ºæ–‡å¯¹è±¡
            
        Returns:
            è®ºæ–‡ä¿¡æ¯å­—å…¸
        """
        # å®‰å…¨èŽ·å–å­—æ®µçš„è¾…åŠ©å‡½æ•°
        def safe_get(paper_dict, key, default=''):
            return paper_dict.get(key, default) if paper_dict else default
        
        bib = paper.get('bib', {})
        
        # æå–ä½œè€…åˆ—è¡¨
        authors = bib.get('author', [])
        if isinstance(authors, list):
            authors_str = '; '.join(authors)
        else:
            authors_str = str(authors)
        
        # æå–å¼•ç”¨æ•°
        num_citations = paper.get('num_citations', 0)
        if num_citations is None:
            num_citations = 0
        
        paper_info = {
            'title': bib.get('title', 'N/A'),
            'authors': authors_str,
            'year': bib.get('pub_year', 'N/A'),
            'venue': bib.get('venue', 'N/A'),
            'publisher': bib.get('publisher', 'N/A'),
            'citations': num_citations,
            'abstract': bib.get('abstract', 'N/A'),
            'url': paper.get('pub_url', 'N/A'),
            'eprint_url': paper.get('eprint_url', 'N/A'),
        }
        
        return paper_info
    
    def sort_by_citations(self, papers: List[Dict], descending=True) -> List[Dict]:
        """
        æŒ‰å¼•ç”¨é‡æŽ’åº
        
        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            descending: æ˜¯å¦é™åºæŽ’åˆ—
            
        Returns:
            æŽ’åºåŽçš„æ–‡çŒ®åˆ—è¡¨
        """
        return sorted(papers, key=lambda x: x['citations'], reverse=descending)
    
    def filter_by_citations(self, papers: List[Dict], min_citations: int = 0) -> List[Dict]:
        """
        æŒ‰æœ€å°å¼•ç”¨é‡ç­›é€‰
        
        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            min_citations: æœ€å°å¼•ç”¨é‡
            
        Returns:
            ç­›é€‰åŽçš„æ–‡çŒ®åˆ—è¡¨
        """
        filtered = [p for p in papers if p['citations'] >= min_citations]
        print(f"ðŸ“Œ ç­›é€‰å¼•ç”¨é‡ >= {min_citations} çš„æ–‡çŒ®: {len(filtered)} ç¯‡")
        return filtered
    
    def export_to_csv(self, papers: List[Dict], filename: str, keyword: str):
        """
        å¯¼å‡ºä¸ºCSVæ–‡ä»¶
        
        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
            keyword: æœç´¢å…³é”®å­—
        """
        if not papers:
            print("âŒ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.dirname(filename)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # å†™å…¥CSV
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['title', 'authors', 'year', 'venue', 'publisher', 
                         'citations', 'abstract', 'url', 'eprint_url']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            writer.writeheader()
            for paper in papers:
                writer.writerow(paper)
        
        print(f"âœ“ æˆåŠŸå¯¼å‡º {len(papers)} ç¯‡æ–‡çŒ®åˆ°: {filename}")
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_statistics(papers, keyword)
    
    def _print_statistics(self, papers: List[Dict], keyword: str):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not papers:
            return
        
        total = len(papers)
        citations = [p['citations'] for p in papers]
        
        print("\n" + "="*60)
        print(f"ðŸ“Š ç»Ÿè®¡ä¿¡æ¯ - å…³é”®å­—: '{keyword}'")
        print("="*60)
        print(f"æ€»æ–‡çŒ®æ•°: {total}")
        print(f"æ€»å¼•ç”¨æ•°: {sum(citations)}")
        print(f"å¹³å‡å¼•ç”¨æ•°: {sum(citations)/total:.1f}")
        print(f"æœ€é«˜å¼•ç”¨æ•°: {max(citations)}")
        print(f"æœ€ä½Žå¼•ç”¨æ•°: {min(citations)}")
        
        # Top 5 é«˜å¼•ç”¨æ–‡çŒ®
        print("\nðŸ† Top 5 é«˜å¼•ç”¨æ–‡çŒ®:")
        for i, paper in enumerate(papers[:5], 1):
            print(f"{i}. [{paper['citations']}æ¬¡] {paper['title']}")
            print(f"   ä½œè€…: {paper['authors'][:100]}...")
            print(f"   å¹´ä»½: {paper['year']}\n")
        
        print("="*60 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python scholar_crawler.py "deep learning"
  python scholar_crawler.py "machine learning" --max 100 --min-citations 50
  python scholar_crawler.py "computer vision" --output results.csv --proxy
        """
    )
    
    parser.add_argument('keyword', type=str, help='æœç´¢å…³é”®å­—')
    parser.add_argument('--max', type=int, default=50, 
                       help='æœ€å¤§èŽ·å–æ–‡çŒ®æ•°é‡ (é»˜è®¤: 50)')
    parser.add_argument('--min-citations', type=int, default=0,
                       help='æœ€å°å¼•ç”¨é‡ç­›é€‰ (é»˜è®¤: 0)')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºCSVæ–‡ä»¶å (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)')
    parser.add_argument('--proxy', action='store_true',
                       help='ä½¿ç”¨ä»£ç† (æŽ¨è)')
    
    args = parser.parse_args()
    
    # ç”Ÿæˆé»˜è®¤è¾“å‡ºæ–‡ä»¶å
    if args.output is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_keyword = args.keyword.replace(' ', '_').replace('/', '_')
        args.output = f"results/{safe_keyword}_{timestamp}.csv"
    
    # åˆ›å»ºçˆ¬è™«å®žä¾‹
    crawler = ScholarCrawler(use_proxy=args.proxy)
    
    # æœç´¢æ–‡çŒ®
    papers = crawler.search_papers(args.keyword, max_results=args.max)
    
    if not papers:
        print("âŒ æœªèŽ·å–åˆ°ä»»ä½•æ–‡çŒ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥æˆ–å°è¯•ä½¿ç”¨ --proxy å‚æ•°")
        return
    
    # æŒ‰å¼•ç”¨é‡æŽ’åº
    papers = crawler.sort_by_citations(papers)
    
    # ç­›é€‰æœ€å°å¼•ç”¨é‡
    if args.min_citations > 0:
        papers = crawler.filter_by_citations(papers, args.min_citations)
    
    # å¯¼å‡ºCSV
    crawler.export_to_csv(papers, args.output, args.keyword)


if __name__ == '__main__':
    main()

