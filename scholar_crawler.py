#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…·
åŠŸèƒ½ï¼šæ ¹æ®å…³é”®å­—æœç´¢æ–‡çŒ®ï¼ŒæŒ‰å¼•ç”¨é‡æ’åºï¼Œå¯¼å‡ºCSV
"""

import csv
import time
import argparse
from datetime import datetime
from typing import List, Dict, Optional
import os
import json

try:
    from scholarly import scholarly, ProxyGenerator
except ImportError:
    print("è¯·å…ˆå®‰è£… scholarly åº“: pip install scholarly")
    exit(1)

try:
    from config import AdvancedSearchConfig
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œå®šä¹‰ä¸€ä¸ªç®€å•ç‰ˆæœ¬
    AdvancedSearchConfig = None


class ScholarCrawler:
    """Google Scholar æ–‡çŒ®çˆ¬å–å™¨"""
    
    def __init__(self, use_proxy=False):
        """
        åˆå§‹åŒ–çˆ¬å–å™¨
        
        Args:
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†ï¼ˆæ¨èä½¿ç”¨ä»¥é¿å…è¢«å°ï¼‰
        """
        self.use_proxy = use_proxy
        if use_proxy:
            self._setup_proxy()
    
    def _setup_proxy(self):
        """è®¾ç½®ä»£ç†"""
        try:
            pg = ProxyGenerator()
            pg.FreeProxies()
            scholarly.use_proxy(pg)
            print("âœ“ ä»£ç†è®¾ç½®æˆåŠŸ")
        except Exception as e:
            print(f"âš  ä»£ç†è®¾ç½®å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨ç›´è¿æ¨¡å¼ï¼Œå¯èƒ½ä¼šé‡åˆ°è®¿é—®é™åˆ¶")
    
    def search_papers(self, keyword: str, max_results: int = 50, 
                     advanced_config: Optional['AdvancedSearchConfig'] = None) -> List[Dict]:
        """
        æœç´¢æ–‡çŒ®ï¼ˆæ”¯æŒé«˜çº§æ£€ç´¢ï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®å­—
            max_results: æœ€å¤§ç»“æœæ•°é‡
            advanced_config: é«˜çº§æ£€ç´¢é…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ–‡çŒ®åˆ—è¡¨
        """
        # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
        if advanced_config:
            search_keyword = advanced_config.to_query_string(keyword)
            print(f"\nğŸ” é«˜çº§æœç´¢æ¨¡å¼")
            print(f"ğŸ“ åŸºç¡€å…³é”®å­—: '{keyword}'")
            print(f"ğŸ” æ„å»ºçš„æŸ¥è¯¢: '{search_keyword}'")
            print(advanced_config)
        else:
            search_keyword = keyword
            print(f"\nğŸ” æ­£åœ¨æœç´¢å…³é”®å­—: '{keyword}'")
        
        print(f"ğŸ“Š ç›®æ ‡è·å–æ•°é‡: {max_results}")
        
        papers = []
        filtered_count = 0
        
        try:
            search_query = scholarly.search_pubs(search_keyword)
            
            for i, paper in enumerate(search_query):
                # å¦‚æœå·²ç»è·å–è¶³å¤Ÿçš„ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ®ï¼Œåˆ™åœæ­¢
                if len(papers) >= max_results:
                    break
                
                # é˜²æ­¢æ— é™å¾ªç¯ï¼ˆæœç´¢çš„æ€»æ•°ä¸è¶…è¿‡max_resultsçš„3å€ï¼‰
                if i >= max_results * 3:
                    print(f"âš  å·²æœç´¢ {i} ç¯‡ï¼Œä½†åªæ‰¾åˆ° {len(papers)} ç¯‡ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ®")
                    break
                
                try:
                    # æå–è®ºæ–‡ä¿¡æ¯
                    paper_info = self._extract_paper_info(paper)
                    
                    # åº”ç”¨é«˜çº§ç­›é€‰
                    if advanced_config and not advanced_config.matches_filters(paper_info):
                        filtered_count += 1
                        continue
                    
                    papers.append(paper_info)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if len(papers) % 10 == 0:
                        print(f"  å·²è·å– {len(papers)} ç¯‡æ–‡çŒ®ï¼ˆå·²ç­›æ‰ {filtered_count} ç¯‡ï¼‰...")
                    
                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¢«å°
                    time.sleep(2)
                    
                except Exception as e:
                    print(f"  âš  å¤„ç†ç¬¬ {i + 1} ç¯‡æ–‡çŒ®æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"âœ“ æˆåŠŸè·å– {len(papers)} ç¯‡æ–‡çŒ®", end='')
            if filtered_count > 0:
                print(f"ï¼ˆç­›é€‰æ‰ {filtered_count} ç¯‡ä¸ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ®ï¼‰")
            else:
                print()
            print()
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
        
        return papers
    
    def _extract_paper_info(self, paper) -> Dict:
        """
        æå–è®ºæ–‡ä¿¡æ¯
        
        Args:
            paper: scholarly è¿”å›çš„è®ºæ–‡å¯¹è±¡
            
        Returns:
            è®ºæ–‡ä¿¡æ¯å­—å…¸
        """
        # å®‰å…¨è·å–å­—æ®µçš„è¾…åŠ©å‡½æ•°
        def safe_get(paper_dict, key, default=''):
            return paper_dict.get(key, default) if paper_dict else default
        
        bib = paper.get('bib', {})
        
        # æå–ä½œè€…åˆ—è¡¨
        authors = bib.get('author', [])
        if isinstance(authors, list):
            authors_str = '; '.join(authors)
        else:
            authors_str = str(authors)
        
        # æå–å¼•ç”¨æ•°
        num_citations = paper.get('num_citations', 0)
        if num_citations is None:
            num_citations = 0
        
        paper_info = {
            'title': bib.get('title', 'N/A'),
            'authors': authors_str,
            'year': bib.get('pub_year', 'N/A'),
            'venue': bib.get('venue', 'N/A'),
            'publisher': bib.get('publisher', 'N/A'),
            'citations': num_citations,
            'abstract': bib.get('abstract', 'N/A'),
            'url': paper.get('pub_url', 'N/A'),
            'eprint_url': paper.get('eprint_url', 'N/A'),
        }
        
        return paper_info
    
    def sort_by_citations(self, papers: List[Dict], descending=True) -> List[Dict]:
        """
        æŒ‰å¼•ç”¨é‡æ’åº
        
        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            descending: æ˜¯å¦é™åºæ’åˆ—
            
        Returns:
            æ’åºåçš„æ–‡çŒ®åˆ—è¡¨
        """
        return sorted(papers, key=lambda x: x['citations'], reverse=descending)
    
    def filter_by_citations(self, papers: List[Dict], min_citations: int = 0) -> List[Dict]:
        """
        æŒ‰æœ€å°å¼•ç”¨é‡ç­›é€‰
        
        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            min_citations: æœ€å°å¼•ç”¨é‡
            
        Returns:
            ç­›é€‰åçš„æ–‡çŒ®åˆ—è¡¨
        """
        filtered = [p for p in papers if p['citations'] >= min_citations]
        print(f"ğŸ“Œ ç­›é€‰å¼•ç”¨é‡ >= {min_citations} çš„æ–‡çŒ®: {len(filtered)} ç¯‡")
        return filtered
    
    def export_to_csv(self, papers: List[Dict], filename: str, keyword: str):
        """
        å¯¼å‡ºä¸ºCSVæ–‡ä»¶
        
        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
            keyword: æœç´¢å…³é”®å­—
        """
        if not papers:
            print("âŒ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.dirname(filename)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # å†™å…¥CSV
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['title', 'authors', 'year', 'venue', 'publisher', 
                         'citations', 'abstract', 'url', 'eprint_url']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            writer.writeheader()
            for paper in papers:
                writer.writerow(paper)
        
        print(f"âœ“ æˆåŠŸå¯¼å‡º {len(papers)} ç¯‡æ–‡çŒ®åˆ°: {filename}")
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_statistics(papers, keyword)
    
    def _print_statistics(self, papers: List[Dict], keyword: str):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not papers:
            return
        
        total = len(papers)
        citations = [p['citations'] for p in papers]
        
        print("\n" + "="*60)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ - å…³é”®å­—: '{keyword}'")
        print("="*60)
        print(f"æ€»æ–‡çŒ®æ•°: {total}")
        print(f"æ€»å¼•ç”¨æ•°: {sum(citations)}")
        print(f"å¹³å‡å¼•ç”¨æ•°: {sum(citations)/total:.1f}")
        print(f"æœ€é«˜å¼•ç”¨æ•°: {max(citations)}")
        print(f"æœ€ä½å¼•ç”¨æ•°: {min(citations)}")
        
        # Top 5 é«˜å¼•ç”¨æ–‡çŒ®
        print("\nğŸ† Top 5 é«˜å¼•ç”¨æ–‡çŒ®:")
        for i, paper in enumerate(papers[:5], 1):
            print(f"{i}. [{paper['citations']}æ¬¡] {paper['title']}")
            print(f"   ä½œè€…: {paper['authors'][:100]}...")
            print(f"   å¹´ä»½: {paper['year']}\n")
        
        print("="*60 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Google Scholar æ–‡çŒ®çˆ¬å–å·¥å…· - æ”¯æŒé«˜çº§æ£€ç´¢',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  åŸºç¡€æœç´¢:
    python scholar_crawler.py "deep learning"
    python scholar_crawler.py "machine learning" --max 100 --min-citations 50
  
  é«˜çº§æ£€ç´¢:
    python scholar_crawler.py "deep learning" --year-start 2020 --year-end 2023
    python scholar_crawler.py "computer vision" --authors "Yann LeCun" --venues "CVPR,ICCV"
    python scholar_crawler.py "NLP" --publishers "ACL" --exclude "survey,review"
    python scholar_crawler.py "AI" --config advanced_search.json
        """
    )
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('keyword', type=str, help='æœç´¢å…³é”®å­—')
    parser.add_argument('--max', type=int, default=50, 
                       help='æœ€å¤§è·å–æ–‡çŒ®æ•°é‡ (é»˜è®¤: 50)')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºCSVæ–‡ä»¶å (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)')
    parser.add_argument('--proxy', action='store_true',
                       help='ä½¿ç”¨ä»£ç† (æ¨è)')
    
    # é«˜çº§æ£€ç´¢å‚æ•°
    advanced_group = parser.add_argument_group('é«˜çº§æ£€ç´¢é€‰é¡¹')
    advanced_group.add_argument('--authors', type=str, default=None,
                               help='ä½œè€…ç­›é€‰ (å¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: "Andrew Ng,Yann LeCun")')
    advanced_group.add_argument('--year-start', type=int, default=None,
                               help='èµ·å§‹å¹´ä»½ (ä¾‹å¦‚: 2020)')
    advanced_group.add_argument('--year-end', type=int, default=None,
                               help='ç»“æŸå¹´ä»½ (ä¾‹å¦‚: 2023)')
    advanced_group.add_argument('--publishers', type=str, default=None,
                               help='å‡ºç‰ˆå•†ç­›é€‰ (å¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: "IEEE,ACM")')
    advanced_group.add_argument('--venues', type=str, default=None,
                               help='ä¼šè®®/æœŸåˆŠç­›é€‰ (å¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: "CVPR,NeurIPS")')
    advanced_group.add_argument('--min-citations', type=int, default=0,
                               help='æœ€å°å¼•ç”¨é‡ (é»˜è®¤: 0)')
    advanced_group.add_argument('--max-citations', type=int, default=None,
                               help='æœ€å¤§å¼•ç”¨é‡ (é»˜è®¤: ä¸é™)')
    advanced_group.add_argument('--exclude', type=str, default=None,
                               help='æ’é™¤å…³é”®å­— (å¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: "survey,review")')
    advanced_group.add_argument('--additional-keywords', type=str, default=None,
                               help='é¢å¤–å…³é”®å­— (å¤šä¸ªç”¨é€—å·åˆ†éš”)')
    advanced_group.add_argument('--keyword-mode', type=str, default='OR',
                               choices=['OR', 'AND'],
                               help='å…³é”®å­—ç»„åˆæ¨¡å¼ (é»˜è®¤: OR)')
    advanced_group.add_argument('--config', type=str, default=None,
                               help='ä»JSONé…ç½®æ–‡ä»¶åŠ è½½é«˜çº§æ£€ç´¢é…ç½®')
    
    args = parser.parse_args()
    
    # ç”Ÿæˆé»˜è®¤è¾“å‡ºæ–‡ä»¶å
    if args.output is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_keyword = args.keyword.replace(' ', '_').replace('/', '_')
        args.output = f"results/{safe_keyword}_{timestamp}.csv"
    
    # æ„å»ºé«˜çº§æ£€ç´¢é…ç½®
    advanced_config = None
    
    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œä»JSONåŠ è½½
    if args.config:
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            if AdvancedSearchConfig:
                advanced_config = AdvancedSearchConfig()
                # ä»å­—å…¸åŠ è½½é…ç½®
                for key, value in config_data.items():
                    if hasattr(advanced_config, key):
                        setattr(advanced_config, key, value)
                print(f"âœ“ å·²ä»é…ç½®æ–‡ä»¶åŠ è½½: {args.config}")
        except Exception as e:
            print(f"âš  åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    # å¦‚æœå‘½ä»¤è¡Œæä¾›äº†é«˜çº§æ£€ç´¢å‚æ•°ï¼Œåˆ›å»ºæˆ–æ›´æ–°é…ç½®
    has_advanced_params = any([
        args.authors, args.year_start, args.year_end,
        args.publishers, args.venues, args.exclude,
        args.additional_keywords, args.min_citations > 0,
        args.max_citations
    ])
    
    if has_advanced_params:
        if not AdvancedSearchConfig:
            print("âš  æ— æ³•ä½¿ç”¨é«˜çº§æ£€ç´¢åŠŸèƒ½ï¼Œè¯·ç¡®ä¿ config.py å¯è®¿é—®")
        else:
            if not advanced_config:
                advanced_config = AdvancedSearchConfig()
            
            # è®¾ç½®é«˜çº§æ£€ç´¢å‚æ•°
            if args.authors:
                advanced_config.authors = [a.strip() for a in args.authors.split(',')]
            if args.year_start:
                advanced_config.year_start = args.year_start
            if args.year_end:
                advanced_config.year_end = args.year_end
            if args.publishers:
                advanced_config.publishers = [p.strip() for p in args.publishers.split(',')]
            if args.venues:
                advanced_config.venues = [v.strip() for v in args.venues.split(',')]
            if args.min_citations > 0:
                advanced_config.citations_min = args.min_citations
            if args.max_citations:
                advanced_config.citations_max = args.max_citations
            if args.exclude:
                advanced_config.exclude_keywords = [e.strip() for e in args.exclude.split(',')]
            if args.additional_keywords:
                advanced_config.additional_keywords = [k.strip() for k in args.additional_keywords.split(',')]
            advanced_config.keyword_mode = args.keyword_mode
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ScholarCrawler(use_proxy=args.proxy)
    
    # æœç´¢æ–‡çŒ®ï¼ˆä½¿ç”¨é«˜çº§æ£€ç´¢é…ç½®ï¼‰
    papers = crawler.search_papers(args.keyword, max_results=args.max, 
                                   advanced_config=advanced_config)
    
    if not papers:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æ–‡çŒ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨ --proxy å‚æ•°")
        return
    
    # æŒ‰å¼•ç”¨é‡æ’åº
    papers = crawler.sort_by_citations(papers)
    
    # å¯¼å‡ºCSV
    crawler.export_to_csv(papers, args.output, args.keyword)


if __name__ == '__main__':
    main()

